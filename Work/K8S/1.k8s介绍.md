# K8S

## K8S Components

### Master

#### kube-apiserver

此 master 组件提供 Kubernetes API。这是Kubernetes控制平台的前端（front-end），可以水平扩展（通过部署更多的实例以达到性能要求）。kubectl / kubernetes dashboard / kuboard 等Kubernetes管理工具就是通过 kubernetes API 实现对 Kubernetes 集群的管理。

#### kube-scheduler

此 master 组件监控所有新创建尚未分配到节点上的 Pod，并且自动选择为 Pod 选择一个合适的节点去运行。

#### kube-controller-manager

负责维护集群状态,例如故障检测,自动扩展,滚动更新等,管理多个Controller
- NodeController： 负责监听节点停机的事件并作出对应响应
- ServiceController： 负责为集群中每一个 副本控制器对象（Replication Controller Object）维护期望的 Pod 副本数
- EndpointsController：负责为端点对象（Endpoints Object，连接 Service 和 Pod）赋值
- Service Account & Token Controller： 负责为新的名称空间创建 default Service Account 以及 API Access Token
- DeploymentController

### Node

#### Container Runtime

负责镜像管理以及Pod和容器真正的运行(CRI)
####  kubelet

此组件是运行在每一个集群节点上的代理程序。它确保 Pod 中的容器处于运行状态。Kubelet 通过多种途径获得 PodSpec 定义，并确保 PodSpec 定义中所描述的容器处于运行和健康的状态。Kubelet不管理不是通过 Kubernetes 创建的容器。

#### kube-proxy

kube-proxy 是一个网络代理程序，运行在集群中的每一个节点上，是实现 Kubernetes Service 概念的重要部分。

kube-proxy 在节点上维护网络规则。这些网络规则使得您可以在集群内、集群外正确地与 Pod 进行网络通信。

### Others

#### etcd

可信赖的分布式键值存储服务,支持最终一致性和高可用，Kubernetes集群的所有配置信息都存储在 etcd 中
- 三个节点可以高可用一次
- 五个节点可以高可用两次
多个节点并不会提升性能,只能提升稳定性

#### docker containerd

#### addon-flanneld

CoreOS团队针对Kubernetes设计的网络规划服务,让集群中中不同节点主机创建的container具有全集群唯一的虚拟IP,并且在此之上建立一个覆盖网络(OverlayNetwork).通过这个网络可以将数据包传递到目标容器内.
#### Ingress Controller

为Kubernetes中的服务提供外网入口
#### addon-CoreDNS

为整个集群提供DNS服务

#### Prometheus
为整个集群提供资源监控能力
#### Federation
跨可用区的集群,提供不同数据中心的K8S集群管理能力 (避免切换kube config)

#### addon-Dashbord
提供B/S访问体系,运行用户通过web进行访问 (rancher)


## Kubernetes

### 对象

- **ReplicationController**
- **ReplicaSet**
- **Deployment**
- **Service**
- **Ingress**
- **ConfigMap**
- **Secret**
- **volume**
- **PersistVolume**
- **DaemonSet**
- **StateFulSet**
- **Job**
- **CronJob**
- **Horizontal Pod Autoscaling** 

#### ReplicationController

用来确保容器应用的数量与用户定义的副本一致
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: php-redis
        image: wangyanglinux/myapp:v1
        env:
        - name: GET_HOSTS_FROM
          value: dns
          name: zhangsan
          value: "123"
        ports:
        - containerPort: 80
```

#### ReplicaSet

与ReplicationController相同,支持集合式selector,官方建议使用ReplicaSet代替ReplicationController

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: myapp
        image: wangyanglinux/myapp:v1
        env:
        - name: GET_HOSTS_FROM
          value: dns
        ports:
        - containerPort: 80
```

#### Deployment 

Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义 (declarative) 方法，用来替代以前的ReplicationController 来方便的管理应用。典型的应用场景包括:

- 定义 Deployment 来创建 Pod 和 ReplicaSet
- 滚动升级和回滚应用
- 扩容和缩容
- 暂停和继续 Deployment

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx-deployment
  namespace: default
  resourceVersion: "100593"
spec:
  progressDeadlineSeconds: 2147483647
  replicas: 3
  revisionHistoryLimit: 4 # RS历史版本
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.11
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 80
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
```

#### Service

- ClusterIp：默认类型，自动分配一个仅 Cluster 内部可以访问的虚拟 IP
- NodePort：在 ClusterIP 基础上为 Service 在每台机器上绑定一个端口，这样就可以通过 <NodeIP>: NodePort 来访问该服务
- LoadBalancer：在 NodePort 的基础上，借助 cloud provider 创建一个外部负载均衡器，并将请求转发到<NodeIP>: NodePort
- ExternalName：把集群外部的服务引入到集群内部来，在集群内部直接使用。没有任何类型代理被创建，这只有 kubernetes 1.7 或更高版本的 kube-dns 才支持

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: default
spec:
  type: ClusterIP
  selector:
    app: myapp
    release: stabel
  ports:
  - name: http
    port: 80
    targetPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deploy
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      release: stabel
  template:
    metadata:
      labels:
        app: myapp
        release: stabel
        env: test
    spec:
      containers:
      - name: myapp
        image: wangyanglinux/myapp:v2
        imagePullPolicy: IfNotPresent
        ports:
        - name: http
          containerPort: 80
```

#### Ingress

**Ingress-Nginx github 地址：**https://github.com/kubernetes/ingress-nginx
**Ingress-Nginx 官方网站：**https://kubernetes.github.io/ingress-nginx/

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-dm
spec:
  replicas: 2
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
        - name: nginx
          image: wangyanglinux/myapp:v1
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  selector:
    name: nginx
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-test
spec:
  rules:
    - host: www1.hongfu.com
      http:
        paths:
        - path: /
          backend:
            serviceName: nginx-svc
            servicePort: 80
```

#### ConfigMap

ConfigMap 功能在 Kubernetes1.2 版本中引入，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。ConfigMap API 给我们提供了向容器中注入配置信息的机制，ConfigMap 可以被用来保存单个属性，也可以用来保存整个配置文件或者 JSON 二进制等对象

1. 使用目录创建ConfigMap
```bash
$ ls docs/user-guide/configmap/kubectl/
    game.file
    ui.file

$ cat docs/user-guide/configmap/kubectl/game.file
version=1.17
name=dave
age=18

$ cat docs/user-guide/configmap/kubectl/ui.properties
level=2
color=yellow


$ kubectl create configmap game-config --from-file=docs/user-guide/configmap/kubectl
```

2. 使用文件创建ConfigMap
3. 使用字面值创建ConfigMap

#### Secret
#### volume
#### PersistVolume
#### DaemonSet

*DaemonSet* 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod

使用 DaemonSet 的一些典型用法： 

- 运行集群存储 daemon，例如在每个 Node 上运行 `glusterd`、`ceph`
- 在每个 Node 上运行日志收集 daemon，例如`fluentd`、`logstash`
- 在每个 Node 上运行监控 daemon，例如 [Prometheus Node Exporter](https://github.com/prometheus/node_exporter)、`collectd`、Datadog 代理、New Relic 代理，或 Ganglia `gmond`

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: deamonset-example
  labels:
    app: daemonset
spec:
  selector:
    matchLabels:
      name: deamonset-example
  template:
    metadata:
      labels:
        name: deamonset-example
    spec:
      containers:
      - name: daemonset-example
        image: wangyanglinux/myapp:v1
```

#### Job

Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束

特殊说明:

- spec.template格式同Pod
- RestartPolicy仅支持Never或OnFailure
- 单个Pod时，默认Pod成功运行后Job即结束
- `.spec.completions`标志Job结束需要成功运行的Pod个数，默认为1
- `.spec.parallelism`标志并行运行的Pod的个数，默认为1
- `.spec.activeDeadlineSeconds`标志失败Pod的重试最大时间，超过这个时间不会继续重试
- job的资源清单不能更改后重新应用

```python
# -*- coding: utf-8 -*-
from __future__ import division
# 导入时间模块
import time
# 计算当前时间
time1=time.time()
# 算法根据马青公式计算圆周率 #
number = 1000
# 多计算10位，防止尾数取舍的影响
number1 = number+10
# 算到小数点后number1位
b = 10**number1
# 求含4/5的首项
x1 = b*4//5
# 求含1/239的首项
x2 = b // -239
# 求第一大项
he = x1+x2
#设置下面循环的终点，即共计算n项
number *= 2
#循环初值=3，末值2n,步长=2
for i in xrange(3,number,2):
  # 求每个含1/5的项及符号
  x1 //= -25
  # 求每个含1/239的项及符号
  x2 //= -57121
  # 求两项之和
  x = (x1+x2) // i
  # 求总和
  he += x
# 求出π
pai = he*4
#舍掉后十位
pai //= 10**10
# 输出圆周率π的值
paistring=str(pai)
result=paistring[0]+str('.')+paistring[1:len(paistring)]
print result
time2=time.time()
print u'Total time:' + str(time2 - time1) + 's'
```

```Dockerfile
FROM hub.c.163.com/public/python:2.7
ADD ./main.py /root
CMD /usr/bin/python /root/main.py
```
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  completions: 6 # 总共运行6个pod
  parallelism: 2 # 同时运行2个
  template:
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: pi:v1
      restartPolicy: Never
```

```shell
cat > main.py<<EOF
# -*- coding: utf-8 -*-
from __future__ import division
# 导入时间模块
import time
# 计算当前时间
time1=time.time()
# 算法根据马青公式计算圆周率 #
number = 1000
# 多计算10位，防止尾数取舍的影响
number1 = number+10
# 算到小数点后number1位
b = 10**number1
# 求含4/5的首项
x1 = b*4//5
# 求含1/239的首项
x2 = b // -239
# 求第一大项
he = x1+x2
#设置下面循环的终点，即共计算n项
number *= 2
#循环初值=3，末值2n,步长=2
for i in xrange(3,number,2):
  # 求每个含1/5的项及符号
  x1 //= -25
  # 求每个含1/239的项及符号
  x2 //= -57121
  # 求两项之和
  x = (x1+x2) // i
  # 求总和
  he += x
# 求出π
pai = he*4
#舍掉后十位
pai //= 10**10
# 输出圆周率π的值
paistring=str(pai)
result=paistring[0]+str('.')+paistring[1:len(paistring)]
print result
time2=time.time()
print u'Total time:' + str(time2 - time1) + 's'
EOF

cat > Dockerfile<<EOF
FROM hub.c.163.com/public/python:2.7
ADD ./main.py /root
CMD /usr/bin/python /root/main.py
EOF

# 构建镜像
docker build -t pi:v1 .

# 打包镜像
docker save -o pi.tar pi:v1
sudo chown ec2-user:ec2-user pi.tar

for i in {1,2,6}; do scp pi.tar ec2-user@node$i:/home/ec2-user/; done

# 在各个node上加载镜像
docker load -i pi.tar

kubectl apply -f job.yaml

kubectl get pods
NAME       READY   STATUS      RESTARTS   AGE
pi-s8ptg   0/1     Completed   0          4m33s

kubectl logs pi-s8ptg
3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989
Total time:0.00218391418457s

kubectl delete job pi
```


#### CronJob

- `.spec.schedule`：调度，必需字段，指定任务运行周期，格式同 Cron
- `.spec.jobTemplate`：Job 模板，必需字段，指定需要运行的任务，格式同 Job
- `.spec.startingDeadlineSeconds` ：启动 Job 的期限（秒级别），该字段是可选的。如果因为任何原因而错过了被调度的时间，那么错过执行时间的 Job 将被认为是失败的。如果没有指定，则没有期限
- `.spec.concurrencyPolicy`：并发策略，该字段也是可选的。它指定了如何处理被 Cron Job 创建的 Job 的并发执行。只允许指定下面策略中的一种：
  - `Allow`（默认）：允许并发运行 Job
  - `Forbid`：禁止并发运行，如果前一个还没有完成，则直接跳过下一个
  - `Replace`：取消当前正在运行的 Job，用一个新的来替换
- `.spec.suspend` ：挂起，该字段也是可选的。如果设置为 `true`，后续所有执行都会被挂起。它对已经开始执行的 Job 不起作用。默认值为 `false`。
- `.spec.successfulJobsHistoryLimit` 和 `.spec.failedJobsHistoryLimit` ：历史限制，是可选的字段。它们指定了可以保留多少完成和失败的 Job。默认情况下，它们分别设置为 `3` 和 `1`。设置限制的值为 `0`，相关类型的 Job 完成后将不会被保留。

**注意，当前策略只能应用于同一个 Cron Job 创建的 Job。如果存在多个 Cron Job，它们创建的 Job 之间总是允许并发运行。**

*Cron Job* 管理基于时间的 Job，即：

- 在给定时间点只运行一次
- 周期性地在给定时间点运行

**典型的用法如下所示：** 
- 在给定的时间点调度 Job 运行
- 创建周期性运行的 Job，例如：数据库备份、发送邮件

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
```

```shell
kubectl get cronjob,job,pod
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/hello   */1 * * * *   False     1        11s             2m9s

NAME                         COMPLETIONS   DURATION   AGE
job.batch/hello-1644144720   1/1           3s         65s
job.batch/hello-1644144780   1/1           2s         5s

NAME                         READY   STATUS      RESTARTS   AGE
pod/hello-1644144720-52s89   0/1     Completed   0          65s
pod/hello-1644144780-qgwnw   0/1     Completed   0          5s

kubectl logs pod/hello-1644144780-qgwnw
Sun Feb  6 10:53:08 UTC 2022
Hello from the Kubernetes cluster

kubectl edit cronjob hello
suspend: false # 改为true 暂停cronjob

kubectl delete cronjob hello
```

#### StatefulSet

StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序

StatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计），其应用场景包括：

- 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现
- 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现
- 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现 1 > 2 > 3
- 有序收缩，有序删除（即从N-1到0） 3 > 2 > 1

#### HPA

应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让 service 中的Pod个数自动调整呢？这就有赖于 HPA （Horizontal Pod Autoscaling）了，顾名思义，使 Pod 水平自动缩放

### 资源

#### NameSpace级别
工作负载资源: Pod,ReplicaSet,Deployment
服务发现资源: Service,Ingress
配置与存储类资源: Volume, CSI
特殊类型资源: ConfigMap,Secret
#### 集群级资源
NameSpace,Node,ClusterRole,ClusterRoleBinding

#### 元数据资源
HPA,PodTemplate,LimitRange

### 容器生命周期
pause: 初始化网络栈 共享网络卷
initC: 可以具有多个initC,线性启动,不跟随pod生命周期,必须全部成功才会启动容器,失败会重启除非`restartPolicy`为`Never`
- 角色分离:比如下载文件,分离创建和部署角色
- 工具封装:wget实用工具
- 延时启动:initC1 中休眠,initC2等待initC1完成
- 与mainC分离: 具有不同文件视图,可以访问Secret
```yaml
##### init 模板
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image:  busybox:1.34.1
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image:  busybox:1.34.1
    # 尝试解析 myservice
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.34.1
    # 尝试解析 mydb
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
```
```yaml
kind: Service
apiVersion: v1
metadata:
  name: myservice
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
---
kind: Service
apiVersion: v1
metadata:
  name: mydb
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9377
```
Init会等待service创建后,才能完成
```shell
kubectl apply -f service.yaml
pod/myapp-pod created

kubectl get pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          2m39s

kubectl apply -f service.yaml 
service/myservice created

NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          8m3s

kubectl describe pod myapp-pod
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m35s  default-scheduler  Successfully assigned default/myapp-pod to node1
  Normal  Pulling    9m34s  kubelet, node1     Pulling image "busybox:1.34.1"
  Normal  Pulled     9m32s  kubelet, node1     Successfully pulled image "busybox:1.34.1"
  Normal  Created    9m32s  kubelet, node1     Created container init-myservice
  Normal  Started    9m32s  kubelet, node1     Started container init-myservice
  Normal  Pulled     4m25s  kubelet, node1     Container image "busybox:1.34.1" already present on machine
  Normal  Created    4m25s  kubelet, node1     Created container init-mydb
  Normal  Started    4m25s  kubelet, node1     Started container init-mydb
  Normal  Pulled     4m19s  kubelet, node1     Container image "busybox:1.34.1" already present on machine
  Normal  Created    4m19s  kubelet, node1     Created container myapp-container
  Normal  Started    4m19s  kubelet, node1     Started container myapp-container
```

钩子: 每个容器有自己的独立的钩子
钩子类型:
- exec
- http

- 启动前钩子:不保证命令完成后才启动容器
- 关闭前钩子:保证命令完成后才关闭容器

探针: 是由kubelet发起的定期诊断,kubelet是随node水平扩展的,只负责自己node的探测.返回值有3个:正常,失败,未知
- 探测形式:
    - TCP: 端口是否有响应
    - HTTP: 返回码范围, 200 <= code <= 400
    - 脚本: 返回值是否为0
- 探针分类:
    - 就绪性探针: 如果返回正常,代表容器就绪.如果返回未知,保持未就绪.适合有顺序的pod启动
    - 存活性探针: 如果正常就静默,不正常就重启.如果返回未知,探针保持存活.

mainC1: 主容器,可以多个,端口不能重合
mainC2: 主容器,可以多个,端口不能重合


