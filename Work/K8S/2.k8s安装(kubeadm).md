
官方文档: https://kubernetes.io/zh/docs/setup/production-environment/ 


### 实验环境

实验环境为3台AWS EC2,具有public IP,并在public subnet.
```
54.216.84.175 master 10.100.0.238 ssh -i integration_sftp.pem ec2-user@54.216.84.175
52.49.101.125 node1 10.100.0.154 ssh -i integration_sftp.pem ec2-user@52.49.101.125
54.246.26.253 node2 10.100.0.61 ssh -i integration_sftp.pem ec2-user@54.246.26.253
```

### 查看内核版本
```
uname -r
```

### 修改主机名
```shell

$ sudo vim /etc/hostname
$ sudo hostnamectl set-hostname <newhostname>
```

### 添加hosts
```shell
$ sudo vim /etc/hosts
10.100.0.238 k8s-master1 master
10.100.0.154 k8s-node1 node1
10.100.0.61 k8s-node2 node2
```

### 上传密钥
```shell
scp -i .ssh/id_rsa .ssh/id_rsa ec2-user@master:/home/ec2-user/.ssh/
scp -i .ssh/id_rsa .ssh/id_rsa ec2-user@node1:/home/ec2-user/.ssh/
scp -i .ssh/id_rsa .ssh/id_rsa ec2-user@node2:/home/ec2-user/.ssh/
```

### 安装依赖包(每台机器上执行)
```shell
sudo yum install -y conntrack ntpdate ntp ipvsadm ipset iptables curl sysstat libseccomp wget vim net-tools git 

sudo modprobe br_netfilter

# 检查内核版本，如果内核版本超过4.19，则需要安装 nf_conntrack 而不是 nf_conntrack_ipv4
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
# modprobe -- nf_conntrack_ipv4
EOF

sudo chmod 755 /etc/sysconfig/modules/ipvs.modules && sudo bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack

sudo yum install -y yum-utils device-mapper-persistent-data lvm2
sudo amazon-linux-extras install -y epel
```

### docker相关(每台机器上执行)
```shell
sudo yum install -y docker 
docker --version

## 创建 /etc/docker 目录
mkdir /etc/docker

# 配置 daemon.
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "insecure-registries": ["harbor.example.com"]
}
EOF
mkdir -p /etc/systemd/system/docker.service.d

# 重启docker服务
systemctl daemon-reload && systemctl restart docker && systemctl enable docker
```

### 关闭防火墙,swap并设置IPtables(对AWS EC2不适用)
```shell
# 关闭防火墙
$ sudo systemctl stop firewalld && systemctl disable firewalld

# 安装并启用iptables
$ sudo yum install -y iptables-services && sudo systemctl start iptables && sudo systemctl enable iptables

# 临时关闭selinux 并修改配置文件
$ sudo setenforce 0 && sudo sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config

$ sudo swapoff -a && sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
```

### 调整内核参数(每台机器上执行)
```shell
cat > kubernetes.conf <<EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1 # 数据包转发
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0 #禁止使用swap空间,只有当系统O0M时才允许使用
vm.overcommit_memory=1 #不检查物理内存是否够用
vm.panic_on_oom=0 #开启OOM
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=1048576
fs.file-max=52706963
fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF

sudo cp kubernetes.conf /etc/sysctl.d/kubernetes.conf
sudo sysctl -p /etc/sysctl.d/kubernetes.conf
```

### kube相关,15.1 版本(每台机器上执行)

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

```shell
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
yum -y  install  kubeadm-1.15.1 kubectl-1.15.1 kubelet-1.15.1
systemctl enable kubelet.service

kubeadm config print init-defaults > kubeadm-config.yaml
```

修改kubeadm-config.yaml
```yaml
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.100.0.238
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: master
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.15.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

启动kubeadm
```shell
# kubeadm-init.log 为安装日志; kubeadm-config.yaml 为初始化配置
kubeadm init --config=kubeadm-config.yaml --experimental-upload-certs | tee kubeadm-init.log

# kube config
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 节点加入
kubeadm join 10.100.0.238:6443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:14d4c10bc80feb2290a76d709963e57f719d5adeab205b89d7520b4d81dac93f

# 如果token 过期后,在 master 节点执行,创建token
kubeadm token create --print-join-command

# 查看token
kubeadm token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
j8eck0.u5vu8pxb63eq9bxb   1h          2020-12-05T21:49:44+08:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token
# 获取证书的hash
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | awk '{print $2}'
6cdf0e0c66e095beca8b7eadb74da19aa904b45e2378bf53172c7f633c0dc9e8

kubeadm join 10.4.7.59:6443 --token 2so6nx.78jpftqpv2g4skvp \
>     --discovery-token-ca-cert-hash sha256:a3220df2e016cbdd41c72d584d8df0a9380954983364d104017a55354116041c \

kubeadm alpha certs check-expiration # 查看当前证书过期时间
```

部署flannel网络插件

[kube-flannel.yaml](../yaml/kube-flanner.yaml)
```shell
mkdir -p /usr/local/kubernetes/cni/flannel && cd /usr/local/kubernetes/cni/flannel
curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml && kubectl apply -f kube-flannel.yml

kubectl get nodes # 检查节点
```

### 



### 文件说明
- kubeadm-init.log 为安装日志
- kubeadm-config.yaml 为初始化配置
- /etc/kubernetes/pki 证书路径
- /var/lib/kubelet/config.yaml kubelet配置文件
- /var/lib/kubelet/kubeadm-flags.env kubelet环境配置文件
- /etc/kubernetes admin controller-manager kubelet scheduler 配置文件


### 相关问题

#### master不调度pod

```shell
kubectl get nodes # 检查节点

kubectl describe node master # 检查master,可以看到有污点
Taints:             node-role.kubernetes.io/master:NoSchedule
```

#### [How to resolve scheduler and controller-manager unhealthy state in Kubernetes](https://stackoverflow.com/questions/64296491/how-to-resolve-scheduler-and-controller-manager-unhealthy-state-in-kubernetes)

```
kubectl get cs
NAME                 STATUS      MESSAGE                                                                                     ERROR
controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused   
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused   
etcd-0               Healthy     {"health":"true"}
```

Modify the following files on all master nodes:

```yaml
$ sudo vi /etc/kubernetes/manifests/kube-scheduler.yaml
```

Clear the line (spec->containers->command) containing this phrase: - --port=0

```yaml
$ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml
```

Clear the line (spec->containers->command) containing this phrase: - --port=0

```yaml
$ sudo systemctl restart kubelet.service
```